---
title: "powerTCR"
author: "Hillary Koch"
package: powerTCR
abstract: >
    This vignette walks through the functionality of the powerTCR package for analyzing the T cell receptor repertoire. The package's main goals are to fit models to the clone size distribution of the TCR repertoire, and to perform model-based comparative analysis of samples, following the work of __my paper__. For user simplicity, powerTCR can handle data in the formats MiTCR, MiTCR with UMIs, MiGEC, VDJtools, ImmunoSEQ (both old and new), MiXCR, and IMSEQ. If not using one of these formats, powerTCR only requires that the user be able to turn a sample TCR repertoire into a vector of clone sizes.
output: BiocStyle::html_document
vignette: > 
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

# Introduction
The powerTCR package allows users to implement the model-based methods discussed in __my paper__. Specifically, the clone size distribution of the T cell receptor (TCR) repertoire exhibits imperfect power law behavior; powerTCR supports a model that keeps this fact in mind. Additionally, powerTCR contains tools to fit another power law model for the TCR repertoire detailed in Desponds et al. (2016). Given a collection of sampled TCR repertoires, powerTCR equips the user with tools for comparative analysis of the samples, using one of two model-based approaches. This leads to hierarchical clustering of the samples to determine their relatedness based on the clone size distribution alone.

## Summary of features

- Read in and parse TCR sequencing files stored in various formats into the necessary format for powerTCR
- Fit two power law-based models to the clone size distribution of the TCR repertoire
    1. The discrete gamma-GPD spliced threshold model of __my paper__
    2. The type-I Pareto model of Desponds et al. (2016)
- Compare sample TCR repertoire samples based on model fit with hierarchical clustering
- Simulate data according from the gamma-GPD spliced threshold distribution

# Fitting a model

In order to fit a model with powerTCR, you only need to be able to supply a __vector of counts__ (that is, a vector of clone sizes). If your data are in a format supported by $\texttt{parseFile}$ or $\texttt{parseFolder}$, you can simply read in your file using one of those functions, specify whether or not you want to use only in-frame sequences, and powerTCR will automatically give you a sorted vector of clone sizes for each sample. This functionality is a wrapper for parsing functions found in the `r CRANpkg("tcR")` package.

powerTCR contains a toy data set, called $\texttt{repertoires}$, with two TCR repertoire samples, which we will use throughout this vignette. You can load powerTCR and this data set by typing:

```{r, echo = 4:5, message = FALSE, warning = FALSE}
# This installs packages from BioConductor
# source("https://bioconductor.org/biocLite.R")
# biocLite("powerTCR")
library(powerTCR)
data("repertoires")
```

$\texttt{repertoires}$ is a list with 2 elements in it, each corresponding to a sample repertoire. Have a look:

```{r}
str(repertoires)
```

These samples are smaller than one might expect a TCR repertoire to be in practice, but for the sake of exploring powerTCR, they permit much faster computation. 

## The discrete gamma-GPD spliced threshold model

The main model that powerTCR focuses on is the discrete gamma-GPD spliced threshold model. This distribution has probability mass function

\[
  	f(x) =
  	\begin{cases}
      	(1-\phi)\frac{h(x|\boldsymbol{\theta}_b)} {H(u-1|\boldsymbol{\theta}_b)}  & \text{for $x \leq u-1$} \\
		\phi g(x|\boldsymbol{\theta}_t, u) & \text{for $x \geq u$}
  	\end{cases},
\]

where $h$ and $H$ are the density and distribution function of a gamma distribution, and $g$ is the density of a generalized Pareto distribution, or GPD. The gamma distribution has density

\[
    h(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}
\]

and the GPD has density

\[
    g(x) = \frac{1}{\sigma}\big(1+\xi \frac{x-u}{\sigma}\big)^{-(1/\xi +1)}.
\]

We can fit the model to each of the samples in $\texttt{repertoires}$ using the function $\texttt{fdiscgammagpd}$. This function takes a few arguments. The most important are as follows:

First, $\texttt{fdiscgammagpd}$ needs to be passed a sample TCR repertoire as a vector of counts. Second, you need to specify a grid of possible thresholds (that is, the parameter $u$) that you are interested in considering. One easy way to do this might be to specify a series of quantiles of the vector of counts. Finally, you also need to specify the shift, which for each sample is ideally the smallest count (at least for TCR repertoire samples). The shift is the minimum value in the support of the distribution, and for clone sizes, should never be smaller than 1.

Let's try fitting the model to the data in $\texttt{repertoires}$.


```{r, warning = FALSE, cache=TRUE}
# This will loop through our list of sample repertoires,
# and store a fit in each
fits <- list()
for(i in 1:length(repertoires)){
    # Choose a sequence of possible u for your model fit
    # Ideally, you want to search a lot of thresholds, but for quick
    # computation, we are only going to test 4
    thresholds <- unique(round(quantile(repertoires[[i]], c(.75,.8,.85,.9))))
    
    fits[[i]] <- fdiscgammagpd(repertoires[[i]], useq = thresholds,
                               shift = min(repertoires[[i]]))
}
names(fits) <- names(repertoires)
```

The output for a fit looks like this:
```{r}
# You could also look at the first sample by typing fits[[1]]
fits$samp1
```

Each value of the output is described in the $\texttt{fdiscgammagpd}$ help file, but the most important are

* nllh: the negative log likelihood of the most likely fit, given the threshold you've checked
* mle: the maximum likelihood estimates for $\phi, \alpha, \beta, u, \sigma,$ and $\xi$ respectively

You can also view the likelihoods for every other threshold you checked (in nllhuseq) as well as the output from $\texttt{optim}$ for the ''bulk''  (truncated gamma) and ''tail''  (GPD) parts of the distribution.

## The Type-I Pareto model

For reproducibility purposes, powerTCR also provides a means to fit the model of Desponds et al. (2016). This model is investigated and discussed in __my paper__. The model follows a type-I Pareto distribution, with density:

\[
f(x) = \frac{\alpha u^\alpha}{x^{\alpha+1}}.
\]

For a given threshold $u$, the estimate for parameter $\alpha$ is computed directly as 

\[
\alpha=n\bigg[\sum_{i=1}^n\text{log}\frac{x_i}{u}\bigg]^{-1}+1
\]

where $n$ is the number of clones with size larger than $u$. This value is computed for every possible threshold $u$, and then the parameters that minimize the KS-statistic between empirical and theoretical distributions are chosen.

Let's fit this model to the $\texttt{repertoires}$ data, and have a look at the output for the first sample.

```{r, cache=T}
desponds_fits <- list()
for(i in 1:length(repertoires)){
    desponds_fits[[i]] <- fdesponds(repertoires[[i]])
}
names(desponds_fits) <- names(repertoires)
desponds_fits$samp1
```

Here, min.KS is the minimum KS-statistic of all possible fits. Cmin is the threshold $u$ that corresponds to the best fit. powerlaw.exponent and pareto.alpha are effectively the same -- pareto.alpha = powerlaw.exponent-1. This is just user preference; for the Pareto density given above, pareto.alpha corresponds to the $\alpha$ shown there. However, if the user is more familiar with a "power law" distribution, then powerlaw.exponent is the parameter they should look at.

# Density, distribution, and quantile functions, plus simulating data
powerTCR provides standard functions to compute the density, distribution, and quantile functions of the discrete gamma-GPD threshold model, as well a function to simulate data. These can be very useful for tasks such as visualizing model fit and conducting a simulation study. The functions behave exactly like popularly used functions such as, say, $\texttt{dnorm, pnorm, qnorm,}$ and $\texttt{rnorm}$. In order to use these functions, you need to specify all of the model parameters. The one exception is $\phi$, which can go unspecified -- details about how $\phi$ defaults are in the help file for $\texttt{ddiscgammagpd}$.

Here, we will use $\texttt{qdiscgammagpd}$ to compute quantiles from the two theoretical distributions we fit above.

```{r}
# The number of clones in each sample
n1 <- length(repertoires[[1]])
n2 <- length(repertoires[[2]])

# Grids of quantiles to check
# (you want the same number of points as were observed in the sample)
q1 <- seq(n1/(n1+1), 1/(n1+1), length.out = n1)
q2 <- seq(n2/(n2+1), 1/(n2+1), length.out = n2)

# Compute the value of fitted distributions at grid of quantiles
theor1 <- qdiscgammagpd(q1, fits[[1]]$mle['shape'], fits[[1]]$mle['rate'],
                        fits[[1]]$mle['thresh'], fits[[1]]$mle['sigma'],
                        fits[[1]]$mle['xi'], fits[[1]]$mle['phi'],
                        min(fits[[1]]$x))
theor2 <- qdiscgammagpd(q2, fits[[2]]$mle['shape'], fits[[2]]$mle['rate'],
                        fits[[2]]$mle['thresh'], fits[[2]]$mle['sigma'],
                        fits[[2]]$mle['xi'], fits[[2]]$mle['phi'],
                        min(fits[[2]]$x))
```

Now, let's visualize the fitted and empirical distributions by plotting them together. Here, the black represents the original data, with the quantiles of the theoretical distributions plotted on top in color.

```{r, fig.wide=TRUE, echo=2:7}
par(mfrow = c(1,2))
plot(log(repertoires[[1]]), log(1:n1), pch = 16, cex = 2,
     xlab = "log clone size", ylab = "log rank", main = "samp1")
points(log(theor1), log(1:n1), pch = 'x', col = "darkcyan")

plot(log(repertoires[[2]]), log(1:n2), pch = 16, cex = 2,
     xlab = "log clone size", ylab = "log rank", main = "samp2")
points(log(theor2), log(1:n2), pch = 'x', col = "chocolate")
```

The fits look pretty good!

Let's also try simulating data.

```{r}
# Simulate 3 sampled repertoires
set.seed(123)
s1 <- rdiscgammagpd(1000, shape = 3, rate = .15, u = 25, sigma = 15,
                    xi = .5, shift = 1)
s2 <- rdiscgammagpd(1000, shape = 3.1, rate = .14, u = 26, sigma = 15,
                    xi = .6, shift = 1)
s3 <- rdiscgammagpd(1000, shape = 10, rate = .3, u = 45, sigma = 20,
                    xi = .7, shift = 1)
```

__NB:__ it is possible to simulate data according to a distribution that is totally unrealistic. For example, what if you chose a very light-tailed gamma distribution and a comparatively very high threshold, but insisted (using $\phi$) that data be observed above the threshold? Here is what happens:

```{r}
bad <- rdiscgammagpd(1000, shape = 1, rate = 2, u = 25, sigma = 10,
                     xi = .5, shift = 1, phi = .2)
plot(log(sort(bad, decreasing = TRUE)), log (1:1000), pch = 16,
     xlab = "log clone size", ylab = "log rank", main = "bad simulation")
```

Fun, but not too realistic for a clone size distribution. There are several ways to go about finding reasonable parameters to simulate. One intuitive and easy technique is to let real data speak for itself -- use parameters similar to those obtained by fitting a distribution to true TCR repertoire data sets.

# Doing comparative analysis

Following the work in __my paper__, powerTCR provides the tools needed to perform hierarchical clustering of TCR repertoire samples according to their Jensen-Shannon divergence. We can test this out on the 3 TCR repertoires we just simulated. First, we need to fit a model to them. For computational efficiency, let's just supply the true thresholds. Then, we can use $\texttt{JS_spliced}$ to compute the Jensen-Shannon divergence between each pair of theoretical distributions corresponding to each of the TCR samples.

$\texttt{JS_spliced}$ needs to be supplied the fitted model parameters as well as a grid. The grid is important: it is the range over which each distribution gets evaluated. If you are comparing a group of TCR repertoires, the minimum value of your grid should be the smallest clone size across all samples. The upper bound of the grid should be something very large, say 100,000 or more. If you don't select a value large enough, you will not be examining the tail of your fitted distributions sufficiently, and the tail is important! The grid should also contain every integer between its minimum and maximum. For computational efficiency, here the upper bound on our grid is only 10,000.

```{r, cache=TRUE}
# Fit model to the data at the true thresholds
sim_fits <- list("fit1" = fdiscgammagpd(s1, useq = 25),
                 "fit2" = fdiscgammagpd(s2, useq = 26),
                 "fit3" = fdiscgammagpd(s3, useq = 45))

# Compute the pairwise JS distance between 3 fitted models
distances <- matrix(rep(0, length(sim_fits)^2), nrow = length(sim_fits))
colnames(distances) <- rownames(distances) <- c("s1","s2","s3")

grid <- min(c(s1,s2,s3)):10000
for(i in 1:(length(sim_fits)-1)){
    for(j in (i+1):length(sim_fits)){
        distances[i,j] <- JS_spliced(grid,
                                     shiftp = min(sim_fits[[i]]$x),
                                     shiftq = min(sim_fits[[j]]$x),
                                     phip = sim_fits[[i]]$mle['phi'],
                                     phiq = sim_fits[[j]]$mle['phi'],
                                     shapep = sim_fits[[i]]$mle['shape'],
                                     shapeq = sim_fits[[j]]$mle['shape'],
                                     ratep = sim_fits[[i]]$mle['rate'],
                                     rateq = sim_fits[[j]]$mle['rate'],
                                     threshp = sim_fits[[i]]$mle['thresh'],
                                     threshq = sim_fits[[j]]$mle['thresh'],
                                     sigmap = sim_fits[[i]]$mle['sigma'],
                                     sigmaq = sim_fits[[j]]$mle['sigma'],
                                     xip = sim_fits[[i]]$mle['xi'],
                                     xiq = sim_fits[[j]]$mle['xi'])
    }
}

# Distances are symmetric
distances <- distances + t(distances)
```

Let's have a look at the distance matrix we just computed:
```{r}
distances
```

Note that, similarly, the function $\texttt{JS_desponds}$ can be used to compute the Jensen-Shannon divergence between samples if they were fit using $\texttt{fdesponds}$ rather than $\texttt{fdiscgammagpd}$.

We can use this distance matrix to perform hierarchical clustering. This is done easily with the $\texttt{clusterPlot}$ function. $\texttt{clusterPlot}$ is just a wrapper for $\texttt{hclust}$, and takes a matrix of Jensen-Shannon distances like the one we just made, plus a type of linkage. All possible types of linkage are listed in the help file, but we recommend using complete linkage.

```{r, fig.small=TRUE}
clusterPlot(distances, method = "complete")
```

The clustering result is exactly what we might expect. Indeed, we simulated s1 and s2 using very similar parameter settings, so we should expect them to be more closely related to each other than to s3. That is exactly what the dendrogram displays.
